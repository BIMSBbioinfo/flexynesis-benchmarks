---
title: "Collate results make summary benchmark figures"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    code_folding: hide
    theme: united
params:
  workdir: '' 
author: Bora Uyar
---

`r date()`

```{r setup, include=FALSE}
workdir = params$workdir 
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, fig.width = 10, 
                      fig.height = 8)
knitr::opts_knit$set(root.dir = workdir)
library(data.table)
library(ggplot2)
library(ggpubr)
library(pbapply)
library(DT)
library(knitr)
ggplot2::theme_set(ggpubr::theme_pubclean())

combine_dfs <- function(df_list) {
  df_list <- lapply(df_list, as.data.frame)
  cn <- unique(do.call(c, lapply(df_list, colnames)))
  df_combined <- do.call(rbind, lapply(names(df_list), function(x) {
    df <- df_list[[x]]
    missing <- setdiff(cn, colnames(df))
    if(length(missing) > 0) {
      to_add <- do.call(cbind, lapply(missing, function(m) {
        d <- data.frame(rep(NA, nrow(df)))
        colnames(d) <- m
        return(d)
      }))
      df <- cbind(df, to_add)
    }
    df <- df[,cn]
    df[['group']] <- x
    return(df)
  }))
  return(df_combined)
}
```

```{r read_files}
files <- dir(workdir, "stats.csv$", recursive = T)
stats <- do.call(rbind, pbapply::pblapply(files[!grepl('baseline', files)], function(x) {
  dt <- data.table::fread(file.path(workdir, x))
  dt$prefix <- gsub(".stats.csv", '', basename(x))
  return(dt)
}))
# combine with analysis table
analysis_table <- data.table::fread(file.path(workdir, 'analysis_table.csv'))[,-1]
stats <- merge.data.table(analysis_table, stats, by = 'prefix')

# stats from off-the-shelf methods
baseline_stats <- do.call(rbind, pbapply::pblapply(files[grepl('baseline', files)], function(x) {
  dt <- data.table::fread(file.path(workdir, x))
  dt$prefix <- gsub(".baseline.stats.csv", '', basename(x))
  return(dt)
}))
colnames(baseline_stats)[1] <- 'tool'

baseline_stats <- merge.data.table(analysis_table[,c('prefix', 'task', 'target', 'batch', 
                                                     'data_types', 'features_min', 'feature_perc', 'log_transform')],
                                   baseline_stats, by = 'prefix')

stats <- data.table(combine_dfs(list('flexynesis' = stats, 
                                     'baseline' = baseline_stats))) 

# split stats by dataset/task
stats <- split(stats, stats$task)

# split each dataset by target variable 
stats <- lapply(stats, function(dt) {
  dt <- split(dt, dt$var)
  return(dt)
})

stats <- sapply(simplify = F, names(stats), function(task) {
  sapply(simplify = FALSE, names(stats[[task]]), function(var) {
        # for each variable, dcast table 
          dt <- stats[[task]][[var]]
            mdt <- dcast.data.table(dt, 
                                    prefix + target + batch + event + time + tool + gnn_conv + fusion + data_types ~ metric, 
                                    value.var = 'value')
            if("mse" %in% colnames(mdt)) {
              mdt <- mdt[order(mse)]
            } else if ("kappa" %in% colnames(mdt)) {
              mdt <- mdt[order(kappa, decreasing = T)]
            } else if ("cindex" %in% colnames(mdt)) {
              mdt <- mdt[order(cindex, decreasing = T)]
            }
      })
})

```

# Rankings 

```{r, results = 'asis'}
for (dataset in names(stats)){
  cat("## ", dataset, " {.tabset} \n\n")
  for (group in names(stats[[dataset]])) {
    cat("### ",group, "  \n\n")
    print(knitr::kable(unique(stats[[dataset]][[group]]), format = 'pipe'))
    cat("\n\n")
  }
  cat("\n\n")
}
```

